{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create results directory using relative paths\n",
    "PROJECT_DIR = Path('../')\n",
    "RESULTS_DIR = PROJECT_DIR / 'results'\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07801b5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KaggleApiHTTPError",
     "evalue": "403 Client Error.\n\nYou don't have permission to access resource at URL: https://api.kaggle.com/v1/datasets.DatasetApiService/GetDataset. Please make sure you are authenticated if you are trying to access a private resource or a resource requiring consent.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/MLProjectHousePricePrediction/.venv/lib/python3.12/site-packages/kagglehub/exceptions.py:67\u001b[39m, in \u001b[36mhandle_call\u001b[39m\u001b[34m(fn, resource_handle)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/MLProjectHousePricePrediction/.venv/lib/python3.12/site-packages/kagglehub/http_resolver.py:364\u001b[39m, in \u001b[36m_get_current_version.<locals>.<lambda>\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    363\u001b[39m r.dataset_slug = h.dataset\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m dataset = handle_call(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mapi_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataset_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    365\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dataset.current_version_number\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/MLProjectHousePricePrediction/.venv/lib/python3.12/site-packages/kagglesdk/datasets/services/dataset_api_service.py:33\u001b[39m, in \u001b[36mDatasetApiClient.get_dataset\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     31\u001b[39m   request = ApiGetDatasetRequest()\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdatasets.DatasetApiService\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGetDataset\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mApiDataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/MLProjectHousePricePrediction/.venv/lib/python3.12/site-packages/kagglesdk/kaggle_http_client.py:98\u001b[39m, in \u001b[36mKaggleHttpClient.call\u001b[39m\u001b[34m(self, service_name, request_name, request, response_type)\u001b[39m\n\u001b[32m     96\u001b[39m http_response = \u001b[38;5;28mself\u001b[39m._session.send(http_request, **settings)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_response\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/MLProjectHousePricePrediction/.venv/lib/python3.12/site-packages/kagglesdk/kaggle_http_client.py:124\u001b[39m, in \u001b[36mKaggleHttpClient._prepare_response\u001b[39m\u001b[34m(self, response_type, http_response)\u001b[39m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m \u001b[43mhttp_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# Allow client to check header content.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/MLProjectHousePricePrediction/.venv/lib/python3.12/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 403 Client Error: Forbidden for url: https://api.kaggle.com/v1/datasets.DatasetApiService/GetDataset",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKaggleApiHTTPError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkagglehub\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Download latest version\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m path = \u001b[43mkagglehub\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataset_download\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43marunjukir245/boston-housing-dataset\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPath to dataset files:\u001b[39m\u001b[33m\"\u001b[39m, path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/MLProjectHousePricePrediction/.venv/lib/python3.12/site-packages/kagglehub/datasets.py:52\u001b[39m, in \u001b[36mdataset_download\u001b[39m\u001b[34m(handle, path, force_download, output_dir)\u001b[39m\n\u001b[32m     50\u001b[39m h = parse_dataset_handle(handle)\n\u001b[32m     51\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDownloading Dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh.to_url()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ...\u001b[39m\u001b[33m\"\u001b[39m, extra={**EXTRA_CONSOLE_BLOCK})\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m resolved_path, _ = \u001b[43mregistry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataset_resolver\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resolved_path\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/MLProjectHousePricePrediction/.venv/lib/python3.12/site-packages/kagglehub/registry.py:28\u001b[39m, in \u001b[36mMultiImplRegistry.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m impl \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m._impls):\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m impl.is_supported(*args, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     30\u001b[39m         fails.append(\u001b[38;5;28mtype\u001b[39m(impl).\u001b[34m__name__\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/MLProjectHousePricePrediction/.venv/lib/python3.12/site-packages/kagglehub/resolver.py:35\u001b[39m, in \u001b[36mResolver.__call__\u001b[39m\u001b[34m(self, handle, path, force_download, output_dir)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     17\u001b[39m     handle: T,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m     output_dir: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     22\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[32m     23\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Resolves a handle into a path with the requested file(s) and the resource's version number.\u001b[39;00m\n\u001b[32m     24\u001b[39m \n\u001b[32m     25\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     33\u001b[39m \u001b[33;03m        Some cases where version number might be missing: Competition datasource, API-based models.\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     path, version = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_resolve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m     \u001b[38;5;66;03m# Note handles are immutable, so _resolve() could not have altered our reference\u001b[39;00m\n\u001b[32m     43\u001b[39m     register_datasource_access(handle, version)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/MLProjectHousePricePrediction/.venv/lib/python3.12/site-packages/kagglehub/http_resolver.py:125\u001b[39m, in \u001b[36mDatasetHttpResolver._resolve\u001b[39m\u001b[34m(self, h, path, force_download, output_dir)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m build_kaggle_client() \u001b[38;5;28;01mas\u001b[39;00m api_client:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m h.is_versioned():\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m         h = h.with_version(\u001b[43m_get_current_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    127\u001b[39m     cache = Cache(override_dir=output_dir)\n\u001b[32m    128\u001b[39m     dataset_path = cache.load_from_cache(h, path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/MLProjectHousePricePrediction/.venv/lib/python3.12/site-packages/kagglehub/http_resolver.py:364\u001b[39m, in \u001b[36m_get_current_version\u001b[39m\u001b[34m(api_client, h)\u001b[39m\n\u001b[32m    362\u001b[39m     r.owner_slug = h.owner\n\u001b[32m    363\u001b[39m     r.dataset_slug = h.dataset\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m     dataset = \u001b[43mhandle_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataset_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset.current_version_number\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(h, NotebookHandle):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/MLProjectHousePricePrediction/.venv/lib/python3.12/site-packages/kagglehub/exceptions.py:107\u001b[39m, in \u001b[36mhandle_call\u001b[39m\u001b[34m(fn, resource_handle)\u001b[39m\n\u001b[32m     98\u001b[39m     message = (\n\u001b[32m     99\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me.response.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Client Error.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    100\u001b[39m         \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease make sure you specified the correct resource identifiers.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    104\u001b[39m     )\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# Default handling\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m KaggleApiHTTPError(message, response=e.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mKaggleApiHTTPError\u001b[39m: 403 Client Error.\n\nYou don't have permission to access resource at URL: https://api.kaggle.com/v1/datasets.DatasetApiService/GetDataset. Please make sure you are authenticated if you are trying to access a private resource or a resource requiring consent."
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"arunjangir245/boston-housing-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after cleaning: 0\n",
      "Training set: 354 samples\n",
      "Test set: 152 samples\n",
      "Data saved to results/\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset using the path from kagglehub\n",
    "df = pd.read_csv(path + '/BostonHousing.csv')\n",
    "\n",
    "# Handle missing values - fill all numeric columns with median\n",
    "df = df.fillna(df.median())\n",
    "\n",
    "# Verify no missing values\n",
    "print(f\"Missing values after cleaning: {df.isnull().sum().sum()}\")\n",
    "\n",
    "# Define features and target\n",
    "X = df.drop('medv', axis=1)\n",
    "y = df['medv']\n",
    "\n",
    "# Train/test split (70/30)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Save split data\n",
    "X_train.to_csv(RESULTS_DIR / 'X_train.csv', index=False)\n",
    "X_test.to_csv(RESULTS_DIR / 'X_test.csv', index=False)\n",
    "y_train.to_csv(RESULTS_DIR / 'y_train.csv', index=False)\n",
    "y_test.to_csv(RESULTS_DIR / 'y_test.csv', index=False)\n",
    "print(\"Data saved to results/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluation_metrics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation functions defined!\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, Dict, Tuple\n",
    "\n",
    "def evaluate_model(\n",
    "    model: Any,\n",
    "    X_train: Any,\n",
    "    X_test: Any,\n",
    "    y_train: Any,\n",
    "    y_test: Any,\n",
    "    model_name: str\n",
    ") -> Tuple[Dict[str, Any], Any]:\n",
    "    \"\"\"Evaluate model and return metrics\"\"\"\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    metrics: Dict[str, Any] = {\n",
    "        'model_name': model_name,\n",
    "        'train_mse': mean_squared_error(y_train, y_train_pred),\n",
    "        'test_mse': mean_squared_error(y_test, y_test_pred),\n",
    "        'train_rmse': np.sqrt(mean_squared_error(y_train, y_train_pred)),\n",
    "        'test_rmse': np.sqrt(mean_squared_error(y_test, y_test_pred)),\n",
    "        'train_mae': mean_absolute_error(y_train, y_train_pred),\n",
    "        'test_mae': mean_absolute_error(y_test, y_test_pred),\n",
    "        'train_r2': r2_score(y_train, y_train_pred),\n",
    "        'test_r2': r2_score(y_test, y_test_pred),\n",
    "    }\n",
    "    \n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\n",
    "    metrics['cv_r2_mean'] = cv_scores.mean()\n",
    "    metrics['cv_r2_std'] = cv_scores.std()\n",
    "    \n",
    "    return metrics, y_test_pred\n",
    "\n",
    "def print_metrics(metrics: Dict[str, Any]) -> None:\n",
    "    \"\"\"Print model metrics\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Model: {metrics['model_name']}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Train MSE: {metrics['train_mse']:.4f} | Test MSE: {metrics['test_mse']:.4f}\")\n",
    "    print(f\"Train RMSE: {metrics['train_rmse']:.4f} | Test RMSE: {metrics['test_rmse']:.4f}\")\n",
    "    print(f\"Train MAE: {metrics['train_mae']:.4f} | Test MAE: {metrics['test_mae']:.4f}\")\n",
    "    print(f\"Train R¬≤: {metrics['train_r2']:.4f} | Test R¬≤: {metrics['test_r2']:.4f}\")\n",
    "    print(f\"CV R¬≤ (mean¬±std): {metrics['cv_r2_mean']:.4f} ¬± {metrics['cv_r2_std']:.4f}\")\n",
    "\n",
    "    # Overfitting/Underfitting analysis\n",
    "    diff = metrics['train_r2'] - metrics['test_r2']\n",
    "    if diff > 0.1:\n",
    "        print(f\"‚ö†Ô∏è  Overfitting detected (train-test R¬≤ gap: {diff:.4f})\")\n",
    "    elif metrics['train_r2'] < 0.5 and metrics['test_r2'] < 0.5:\n",
    "        print(f\"‚ö†Ô∏è  Underfitting detected (low R¬≤ on both sets)\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Good fit\")\n",
    "\n",
    "print(\"Evaluation functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear_univariate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "1. UNIVARIATE LINEAR REGRESSION (rm ‚Üí medv)\n",
      "============================================================\n",
      "\n",
      "==================================================\n",
      "Model: Linear Regression (Univariate)\n",
      "==================================================\n",
      "Train MSE: 44.9427 | Test MSE: 40.3866\n",
      "Train RMSE: 6.7039 | Test RMSE: 6.3550\n",
      "Train MAE: 4.5033 | Test MAE: 4.3207\n",
      "Train R¬≤: 0.4887 | Test R¬≤: 0.4580\n",
      "CV R¬≤ (mean¬±std): 0.4524 ¬± 0.1773\n",
      "‚ö†Ô∏è  Underfitting detected (low R¬≤ on both sets)\n",
      "\n",
      "‚úÖ Model and predictions saved!\n"
     ]
    }
   ],
   "source": [
    "# 1. UNIVARIATE LINEAR REGRESSION\n",
    "# Using only 'rm' (rooms) - strongest correlation with target\n",
    "print(\"=\"*60)\n",
    "print(\"1. UNIVARIATE LINEAR REGRESSION (rm ‚Üí medv)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "X_train_uni = X_train[['rm']]\n",
    "X_test_uni = X_test[['rm']]\n",
    "\n",
    "lr_uni = LinearRegression()\n",
    "lr_uni.fit(X_train_uni, y_train)\n",
    "\n",
    "metrics_uni, pred_uni = evaluate_model(lr_uni, X_train_uni, X_test_uni, y_train, y_test, \"Linear Regression (Univariate)\")\n",
    "print_metrics(metrics_uni)\n",
    "\n",
    "# Save model and predictions\n",
    "joblib.dump(lr_uni, str(RESULTS_DIR) + '/linear_univariate.joblib')\n",
    "np.save(str(RESULTS_DIR) + '/pred_linear_univariate.npy', pred_uni)\n",
    "\n",
    "# Save metrics\n",
    "with open(str(RESULTS_DIR) + '/metrics_linear_univariate.json', 'w') as f:\n",
    "    json.dump(metrics_uni, f, indent=2)\n",
    "\n",
    "print(\"\\n‚úÖ Model and predictions saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear_multivariate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "2. MULTIVARIATE LINEAR REGRESSION (all features)\n",
      "============================================================\n",
      "\n",
      "==================================================\n",
      "Model: Linear Regression (Multivariate)\n",
      "==================================================\n",
      "Train MSE: 22.5704 | Test MSE: 21.6188\n",
      "Train RMSE: 4.7508 | Test RMSE: 4.6496\n",
      "Train MAE: 3.3590 | Test MAE: 3.1761\n",
      "Train R¬≤: 0.7432 | Test R¬≤: 0.7099\n",
      "CV R¬≤ (mean¬±std): 0.6880 ¬± 0.0923\n",
      "‚úÖ Good fit\n",
      "\n",
      "Top 5 Most Important Features:\n",
      "    feature  coefficient\n",
      "4       nox   -15.423388\n",
      "5        rm     4.056626\n",
      "3      chas     3.121412\n",
      "7       dis    -1.379212\n",
      "10  ptratio    -0.912924\n",
      "\n",
      "‚úÖ Model and predictions saved!\n"
     ]
    }
   ],
   "source": [
    "# 2. MULTIVARIATE LINEAR REGRESSION\n",
    "# Using all features\n",
    "print(\"=\"*60)\n",
    "print(\"2. MULTIVARIATE LINEAR REGRESSION (all features)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "lr_multi = LinearRegression()\n",
    "lr_multi.fit(X_train, y_train)\n",
    "\n",
    "metrics_multi, pred_multi = evaluate_model(lr_multi, X_train, X_test, y_train, y_test, \"Linear Regression (Multivariate)\")\n",
    "print_metrics(metrics_multi)\n",
    "\n",
    "# Feature importance (coefficients)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'coefficient': lr_multi.coef_\n",
    "}).sort_values('coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(\"\\nTop 5 Most Important Features:\")\n",
    "print(feature_importance.head())\n",
    "\n",
    "# Save model and predictions\n",
    "joblib.dump(lr_multi, str(RESULTS_DIR) + '/linear_multivariate.joblib')\n",
    "np.save(str(RESULTS_DIR) + '/pred_linear_multivariate.npy', pred_multi)\n",
    "\n",
    "# Save metrics\n",
    "with open(str(RESULTS_DIR) + '/metrics_linear_multivariate.json', 'w') as f:\n",
    "    json.dump(metrics_multi, f, indent=2)\n",
    "\n",
    "print(\"\\n‚úÖ Model and predictions saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature_selection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "3. FEATURE SELECTION - Top Correlated Features\n",
      "============================================================\n",
      "Selected features: ['lstat', 'rm', 'ptratio', 'indus', 'tax', 'nox']\n",
      "\n",
      "==================================================\n",
      "Model: Linear Regression (Feature Selection)\n",
      "==================================================\n",
      "Train MSE: 27.4869 | Test MSE: 26.0001\n",
      "Train RMSE: 5.2428 | Test RMSE: 5.0990\n",
      "Train MAE: 3.6675 | Test MAE: 3.5702\n",
      "Train R¬≤: 0.6873 | Test R¬≤: 0.6511\n",
      "CV R¬≤ (mean¬±std): 0.6512 ¬± 0.0902\n",
      "‚úÖ Good fit\n",
      "\n",
      "‚úÖ Model and predictions saved!\n"
     ]
    }
   ],
   "source": [
    "# 3. FEATURE SELECTION - Using top correlated features\n",
    "print(\"=\"*60)\n",
    "print(\"3. FEATURE SELECTION - Top Correlated Features\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Select top features based on correlation with target\n",
    "correlations = df.corr()['medv'].drop('medv').abs().sort_values(ascending=False)\n",
    "top_features = correlations.head(6).index.tolist()\n",
    "print(f\"Selected features: {top_features}\")\n",
    "\n",
    "X_train_fs = X_train[top_features]\n",
    "X_test_fs = X_test[top_features]\n",
    "\n",
    "lr_fs = LinearRegression()\n",
    "lr_fs.fit(X_train_fs, y_train)\n",
    "\n",
    "metrics_fs, pred_fs = evaluate_model(lr_fs, X_train_fs, X_test_fs, y_train, y_test, \"Linear Regression (Feature Selection)\")\n",
    "print_metrics(metrics_fs)\n",
    "\n",
    "# Save model and predictions\n",
    "joblib.dump(lr_fs, str(RESULTS_DIR) + '/linear_feature_selection.joblib')\n",
    "np.save(str(RESULTS_DIR) + '/pred_linear_feature_selection.npy', pred_fs)\n",
    "\n",
    "with open(str(RESULTS_DIR) + '/metrics_linear_feature_selection.json', 'w') as f:\n",
    "    json.dump(metrics_fs, f, indent=2)\n",
    "\n",
    "print(\"\\n‚úÖ Model and predictions saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polynomial_regression",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "4. POLYNOMIAL REGRESSION\n",
      "============================================================\n",
      "\n",
      "--- Degree 2 ---\n",
      "\n",
      "==================================================\n",
      "Model: Polynomial Regression (degree=2)\n",
      "==================================================\n",
      "Train MSE: 40.7690 | Test MSE: 32.2518\n",
      "Train RMSE: 6.3851 | Test RMSE: 5.6791\n",
      "Train MAE: 4.2943 | Test MAE: 4.0268\n",
      "Train R¬≤: 0.5362 | Test R¬≤: 0.5672\n",
      "CV R¬≤ (mean¬±std): 0.4829 ¬± 0.2243\n",
      "‚úÖ Good fit\n",
      "\n",
      "--- Degree 3 ---\n",
      "\n",
      "==================================================\n",
      "Model: Polynomial Regression (degree=3)\n",
      "==================================================\n",
      "Train MSE: 39.6337 | Test MSE: 31.1067\n",
      "Train RMSE: 6.2955 | Test RMSE: 5.5773\n",
      "Train MAE: 4.2922 | Test MAE: 3.9042\n",
      "Train R¬≤: 0.5491 | Test R¬≤: 0.5825\n",
      "CV R¬≤ (mean¬±std): 0.4908 ¬± 0.2045\n",
      "‚úÖ Good fit\n",
      "\n",
      "============================================================\n",
      "POLYNOMIAL DEGREE COMPARISON\n",
      "============================================================\n",
      "Degree 2: Train R¬≤=0.5362, Test R¬≤=0.5672, CV R¬≤=0.4829\n",
      "Degree 3: Train R¬≤=0.5491, Test R¬≤=0.5825, CV R¬≤=0.4908\n",
      "\n",
      "‚úÖ Polynomial models saved!\n"
     ]
    }
   ],
   "source": [
    "# 4. POLYNOMIAL REGRESSION\n",
    "print(\"=\"*60)\n",
    "print(\"4. POLYNOMIAL REGRESSION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results_poly = {}\n",
    "\n",
    "for degree in [2, 3]:\n",
    "    print(f\"\\n--- Degree {degree} ---\")\n",
    "    \n",
    "    # Create polynomial features\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_train_poly = poly.fit_transform(X_train_uni)\n",
    "    X_test_poly = poly.transform(X_test_uni)\n",
    "    \n",
    "    lr_poly = LinearRegression()\n",
    "    lr_poly.fit(X_train_poly, y_train)\n",
    "    \n",
    "    metrics_poly, pred_poly = evaluate_model(\n",
    "        lr_poly, X_train_poly, X_test_poly, y_train, y_test, \n",
    "        f\"Polynomial Regression (degree={degree})\"\n",
    "    )\n",
    "    print_metrics(metrics_poly)\n",
    "    \n",
    "    results_poly[degree] = {\n",
    "        'model': lr_poly,\n",
    "        'metrics': metrics_poly,\n",
    "        'predictions': pred_poly,\n",
    "        'poly': poly\n",
    "    }\n",
    "    \n",
    "    # Save model\n",
    "    joblib.dump(lr_poly, f'{RESULTS_DIR}/polynomial_degree{degree}.joblib')\n",
    "    joblib.dump(poly, f'{RESULTS_DIR}/polynomial_transformer_degree{degree}.joblib')\n",
    "    np.save(f'{RESULTS_DIR}/pred_polynomial_degree{degree}.npy', pred_poly)\n",
    "    \n",
    "    with open(f'{RESULTS_DIR}/metrics_polynomial_degree{degree}.json', 'w') as f:\n",
    "        json.dump(metrics_poly, f, indent=2)\n",
    "\n",
    "# Compare degrees\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"POLYNOMIAL DEGREE COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "for degree, data in results_poly.items():\n",
    "    m = data['metrics']\n",
    "    print(f\"Degree {degree}: Train R¬≤={m['train_r2']:.4f}, Test R¬≤={m['test_r2']:.4f}, CV R¬≤={m['cv_r2_mean']:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Polynomial models saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gradient_descent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "5. GRADIENT DESCENT OPTIMIZATION (SGDRegressor)\n",
      "============================================================\n",
      "\n",
      "--- SGD (constant) ---\n",
      "\n",
      "==================================================\n",
      "Model: SGD (constant)\n",
      "==================================================\n",
      "Train MSE: 23.3203 | Test MSE: 22.7980\n",
      "Train RMSE: 4.8291 | Test RMSE: 4.7747\n",
      "Train MAE: 3.4790 | Test MAE: 3.2964\n",
      "Train R¬≤: 0.7347 | Test R¬≤: 0.6940\n",
      "CV R¬≤ (mean¬±std): 0.6657 ¬± 0.0882\n",
      "‚úÖ Good fit\n",
      "\n",
      "--- SGD (adaptive) ---\n",
      "\n",
      "==================================================\n",
      "Model: SGD (adaptive)\n",
      "==================================================\n",
      "Train MSE: 22.6660 | Test MSE: 21.5913\n",
      "Train RMSE: 4.7609 | Test RMSE: 4.6466\n",
      "Train MAE: 3.3544 | Test MAE: 3.1626\n",
      "Train R¬≤: 0.7421 | Test R¬≤: 0.7102\n",
      "CV R¬≤ (mean¬±std): 0.6901 ¬± 0.0900\n",
      "‚úÖ Good fit\n",
      "\n",
      "‚úÖ Gradient descent models saved!\n"
     ]
    }
   ],
   "source": [
    "# 5. GRADIENT DESCENT (SGDRegressor)\n",
    "print(\"=\"*60)\n",
    "print(\"5. GRADIENT DESCENT OPTIMIZATION (SGDRegressor)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Scale features for gradient descent\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# SGDRegressor with different configurations\n",
    "sgd_configs = [\n",
    "    {'loss': 'squared_error', 'learning_rate': 'constant', 'eta0': 0.01, 'name': 'SGD (constant)'},\n",
    "    {'loss': 'squared_error', 'learning_rate': 'adaptive', 'eta0': 0.01, 'name': 'SGD (adaptive)'},\n",
    "]\n",
    "\n",
    "results_sgd = {}\n",
    "\n",
    "for config in sgd_configs:\n",
    "    print(f\"\\n--- {config['name']} ---\")\n",
    "    \n",
    "    sgd = SGDRegressor(\n",
    "        loss=config['loss'],\n",
    "        learning_rate=config['learning_rate'],\n",
    "        eta0=config['eta0'],\n",
    "        max_iter=1000,\n",
    "        random_state=42,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1\n",
    "    )\n",
    "    \n",
    "    sgd.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    metrics_sgd, pred_sgd = evaluate_model(\n",
    "        sgd, X_train_scaled, X_test_scaled, y_train, y_test,\n",
    "        config['name']\n",
    "    )\n",
    "    print_metrics(metrics_sgd)\n",
    "    \n",
    "    results_sgd[config['name']] = {\n",
    "        'model': sgd,\n",
    "        'metrics': metrics_sgd,\n",
    "        'predictions': pred_sgd\n",
    "    }\n",
    "    \n",
    "    # Save model and scaler\n",
    "    safe_name = config['name'].replace(' ', '_').replace('(', '').replace(')', '')\n",
    "    joblib.dump(sgd, f'{RESULTS_DIR}/{safe_name}.joblib')\n",
    "    np.save(f'{RESULTS_DIR}/pred_{safe_name}.npy', pred_sgd)\n",
    "    \n",
    "    with open(f'{RESULTS_DIR}/metrics_{safe_name}.json', 'w') as f:\n",
    "        json.dump(metrics_sgd, f, indent=2)\n",
    "\n",
    "# Save scaler\n",
    "joblib.dump(scaler, f'{RESULTS_DIR}/scaler.joblib')\n",
    "\n",
    "print(\"\\n‚úÖ Gradient descent models saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross_validation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "6. CROSS-VALIDATION ANALYSIS (5-Fold)\n",
      "============================================================\n",
      "\n",
      "Linear (Uni):\n",
      "  R¬≤: 0.4524 ¬± 0.1773\n",
      "  MSE: 46.0120 ¬± 11.4529\n",
      "\n",
      "Linear (Multi):\n",
      "  R¬≤: 0.6880 ¬± 0.0923\n",
      "  MSE: 25.9884 ¬± 4.7246\n",
      "\n",
      "Linear (FS):\n",
      "  R¬≤: 0.6512 ¬± 0.0902\n",
      "  MSE: 29.4805 ¬± 6.5223\n",
      "\n",
      "‚úÖ Cross-validation results saved!\n"
     ]
    }
   ],
   "source": [
    "# 6. CROSS-VALIDATION ANALYSIS\n",
    "print(\"=\"*60)\n",
    "print(\"6. CROSS-VALIDATION ANALYSIS (5-Fold)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "models_to_cv = {\n",
    "    'Linear (Uni)': (lr_uni, X_train_uni),\n",
    "    'Linear (Multi)': (lr_multi, X_train),\n",
    "    'Linear (FS)': (lr_fs, X_train_fs),\n",
    "}\n",
    "\n",
    "cv_results = {}\n",
    "\n",
    "for name, (model, X_data) in models_to_cv.items():\n",
    "    # R¬≤ cross-validation\n",
    "    cv_r2 = cross_val_score(model, X_data, y_train, cv=5, scoring='r2')\n",
    "    \n",
    "    # Negative MSE cross-validation\n",
    "    cv_mse = cross_val_score(model, X_data, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    \n",
    "    cv_results[name] = {\n",
    "        'r2_mean': cv_r2.mean(),\n",
    "        'r2_std': cv_r2.std(),\n",
    "        'mse_mean': -cv_mse.mean(),\n",
    "        'mse_std': cv_mse.std()\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  R¬≤: {cv_r2.mean():.4f} ¬± {cv_r2.std():.4f}\")\n",
    "    print(f\"  MSE: {-cv_mse.mean():.4f} ¬± {cv_mse.std():.4f}\")\n",
    "\n",
    "# Save CV results\n",
    "with open(str(RESULTS_DIR) + '/cv_results.json', 'w') as f:\n",
    "    json.dump(cv_results, f, indent=2)\n",
    "\n",
    "print(\"\\n‚úÖ Cross-validation results saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_comparison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "7. FINAL MODEL COMPARISON\n",
      "============================================================\n",
      "\n",
      "Model Performance Ranking (by Test R¬≤):\n",
      "                           model_name  train_r2  test_r2  train_rmse  test_rmse  cv_r2_mean  cv_r2_std\n",
      "     Linear Regression (Multivariate)  0.743216 0.709866    4.750834   4.649599    0.688038   0.092316\n",
      "Linear Regression (Feature Selection)  0.687281 0.651066    5.242799   5.099033    0.651222   0.090185\n",
      "     Polynomial Regression (degree=3)  0.549087 0.582533    6.295528   5.577342    0.490782   0.204544\n",
      "     Polynomial Regression (degree=2)  0.536170 0.567166    6.385063   5.679065    0.482943   0.224297\n",
      "       Linear Regression (Univariate)  0.488686 0.457993    6.703935   6.355044    0.452441   0.177267\n",
      "\n",
      "üèÜ Best Model: Linear Regression (Multivariate)\n",
      "   Test R¬≤: 0.7099\n",
      "\n",
      "‚úÖ Comparison saved!\n"
     ]
    }
   ],
   "source": [
    "# 7. FINAL MODEL COMPARISON\n",
    "print(\"=\"*60)\n",
    "print(\"7. FINAL MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_metrics = [\n",
    "    metrics_uni,\n",
    "    metrics_multi,\n",
    "    metrics_fs,\n",
    "    results_poly[2]['metrics'],\n",
    "    results_poly[3]['metrics'],\n",
    "]\n",
    "\n",
    "comparison_df = pd.DataFrame(all_metrics)\n",
    "comparison_df = comparison_df[['model_name', 'train_r2', 'test_r2', 'train_rmse', 'test_rmse', 'cv_r2_mean', 'cv_r2_std']]\n",
    "comparison_df = comparison_df.sort_values('test_r2', ascending=False)\n",
    "\n",
    "print(\"\\nModel Performance Ranking (by Test R¬≤):\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Save comparison\n",
    "comparison_df.to_csv(str(RESULTS_DIR) + '/model_comparison.csv', index=False)\n",
    "\n",
    "best_model = comparison_df.iloc[0]['model_name']\n",
    "best_r2 = comparison_df.iloc[0]['test_r2']\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_model}\")\n",
    "print(f\"   Test R¬≤: {best_r2:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Comparison saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-project",
   "language": "python",
   "name": "ml-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
